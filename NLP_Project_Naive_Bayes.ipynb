{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbcd4_UtYxJ6",
        "outputId": "9804390d-a456-4ded-cebf-1c156516ffa6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using MultinomialNB from sklearn"
      ],
      "metadata": {
        "id": "BOiYOXKoih6t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This relies on a standard, prebuilt pipeline for machine learning with CountVectorizer and MultinomialNB to classify text.\n",
        "\n",
        "*   Leverages CountVectorizer to extract features (term frequency counts).\n",
        "*   Automatically handles tokenization and vectorization as part of the pipeline.\n",
        "*   Uses sklearn's prebuilt metric functions for calculation.\n",
        "\n",
        "We expect it to perform faster due to the optimized sklearn library.\n",
        "\n"
      ],
      "metadata": {
        "id": "p_-omoNgjKQ0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXvldsKkX1YA",
        "outputId": "1f190fad-3895-46d0-985e-9e9ff65179bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading datasets...\n",
            "\n",
            "Training and evaluating sklearn model on SemEval dataset...\n",
            "Training completed in 72.35 seconds.\n",
            "\n",
            "Training Set Metrics:\n",
            "Accuracy: 0.7214\n",
            "Precision: 0.7269\n",
            "Recall: 0.7214\n",
            "F1 Score: 0.7214\n",
            "\n",
            "Confusion Matrix:\n",
            "Predicted      0      1\n",
            "Actual                 \n",
            "0          42952  20399\n",
            "1          12962  43444\n",
            "\n",
            "Validation Set Metrics:\n",
            "Accuracy: 0.5144\n",
            "Precision: 0.5144\n",
            "Recall: 0.5144\n",
            "F1 Score: 0.5140\n",
            "\n",
            "Confusion Matrix:\n",
            "Predicted     0     1\n",
            "Actual               \n",
            "0          1213  1287\n",
            "1          1141  1359\n",
            "\n",
            "Test Set Metrics:\n",
            "Accuracy: 0.8413\n",
            "Precision: 0.8480\n",
            "Recall: 0.8413\n",
            "F1 Score: 0.8412\n",
            "\n",
            "Confusion Matrix:\n",
            "Predicted      0      1\n",
            "Actual                 \n",
            "0          14649   1623\n",
            "1           3816  14184\n",
            "\n",
            "Training and evaluating sklearn model on GenAI dataset...\n",
            "Training completed in 42.79 seconds.\n",
            "\n",
            "Training Set Metrics:\n",
            "Accuracy: 0.7434\n",
            "Precision: 0.7656\n",
            "Recall: 0.7434\n",
            "F1 Score: 0.7473\n",
            "\n",
            "Confusion Matrix:\n",
            "Predicted      0      1\n",
            "Actual                 \n",
            "0          35417   9469\n",
            "1          21261  53610\n",
            "\n",
            "Validation Set Metrics:\n",
            "Accuracy: 0.6952\n",
            "Precision: 0.7192\n",
            "Recall: 0.6952\n",
            "F1 Score: 0.6999\n",
            "\n",
            "Confusion Matrix:\n",
            "Predicted      0       1\n",
            "Actual                  \n",
            "0          71911   26417\n",
            "1          53354  110076\n",
            "\n",
            "Test Set Metrics:\n",
            "Accuracy: 0.6943\n",
            "Precision: 0.7183\n",
            "Recall: 0.6943\n",
            "F1 Score: 0.6991\n",
            "\n",
            "Confusion Matrix:\n",
            "Predicted      0      1\n",
            "Actual                 \n",
            "0          16694   6198\n",
            "1          12471  25714\n",
            "\n",
            "Results Summary:\n",
            "\n",
            "SemEval Dataset Metrics:\n",
            "Training Set Metrics: {'accuracy': 0.7214275574705445, 'precision': 0.7268736630389256, 'recall': 0.7214275574705445, 'f1': 0.7213564841591202}\n",
            "Validation Set Metrics: {'accuracy': 0.5144, 'precision': 0.5144492801368635, 'recall': 0.5144, 'f1': 0.5139856046859794}\n",
            "Test Set Metrics: {'accuracy': 0.8412990196078431, 'precision': 0.8479528358586716, 'recall': 0.8412990196078431, 'f1': 0.8411612122989213}\n",
            "\n",
            "GenAI Dataset Metrics:\n",
            "Training Set Metrics: {'accuracy': 0.7433970456841771, 'precision': 0.7655524748623671, 'recall': 0.7433970456841771, 'f1': 0.747326135355602}\n",
            "Validation Set Metrics: {'accuracy': 0.6952490468295143, 'precision': 0.7191636348876905, 'recall': 0.6952490468295143, 'f1': 0.699920925882662}\n",
            "Test Set Metrics: {'accuracy': 0.6943366570067292, 'precision': 0.7183062779451137, 'recall': 0.6943366570067292, 'f1': 0.6990763107542244}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from tqdm import tqdm\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "# Load Data Functions\n",
        "def get_texts_labels(file_path):\n",
        "    texts, labels = [], []\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            record = json.loads(line.strip())\n",
        "            texts.append(record['text'])\n",
        "            labels.append(record['label'])\n",
        "    return texts, labels\n",
        "\n",
        "def get_texts(file_path):\n",
        "    texts = []\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            record = json.loads(line.strip())\n",
        "            texts.append(record['text'])\n",
        "    return texts\n",
        "\n",
        "# Preprocessing Function\n",
        "def preprocess_text(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = text.lower()\n",
        "    text = text.replace('\\n', ' ')\n",
        "    text = re.sub(r'[^a-zA-Z\\s.,!?]', '', text)\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words or word in '.,!?'])\n",
        "    return text\n",
        "\n",
        "# Define file paths\n",
        "file_path = '/content/drive/MyDrive/project/COMP6781_MNBvBERT_MGTDetect-main'\n",
        "SemEval_train_file_path = file_path + '/SemEval_data/subtaskA_train_monolingual.jsonl'\n",
        "SemEval_val_file_path = file_path + '/SemEval_data/subtaskA_dev_monolingual.jsonl'\n",
        "SemEval_test_file_path = file_path + '/SemEval_data/subtaskA_monolingual_test1.jsonl'  # Added SemEval test path\n",
        "GenAI_train_file_path = file_path + '/GenAI_data/en_train.jsonl'\n",
        "GenAI_val_file_path = file_path + '/GenAI_data/en_dev.jsonl'\n",
        "GenAI_test_file_path = file_path + '/GenAI_data/en_test.jsonl'  # Added GenAI test path\n",
        "\n",
        "# Load datasets\n",
        "print(\"Loading datasets...\")\n",
        "SemEval_train_texts, SemEval_train_labels = get_texts_labels(SemEval_train_file_path)\n",
        "SemEval_val_texts, SemEval_val_labels = get_texts_labels(SemEval_val_file_path)\n",
        "SemEval_test_texts, SemEval_test_labels = get_texts_labels(SemEval_test_file_path)  # Load test set\n",
        "\n",
        "GenAI_train_texts, GenAI_train_labels = get_texts_labels(GenAI_train_file_path)\n",
        "GenAI_val_texts, GenAI_val_labels = get_texts_labels(GenAI_val_file_path)\n",
        "\n",
        "# Step 1: Split 10% of the GenAI training set for testing and remove it from the training data\n",
        "GenAI_train_texts, GenAI_test_texts, GenAI_train_labels, GenAI_test_labels = train_test_split(\n",
        "    GenAI_train_texts, GenAI_train_labels, test_size=0.1, stratify=GenAI_train_labels, random_state=42\n",
        ")\n",
        "\n",
        "# Step 2: Stratified Sampling to Match SemEval Dataset Size\n",
        "train_data, _ = train_test_split(\n",
        "    list(zip(GenAI_train_texts, GenAI_train_labels)),\n",
        "    train_size=len(SemEval_train_texts),\n",
        "    stratify=GenAI_train_labels,\n",
        "    random_state=42\n",
        ")\n",
        "GenAI_train_texts, GenAI_train_labels = map(list, zip(*train_data))\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "SemEval_train_labels = label_encoder.fit_transform(SemEval_train_labels)\n",
        "SemEval_val_labels = label_encoder.transform(SemEval_val_labels)\n",
        "SemEval_test_labels = label_encoder.transform(SemEval_test_labels)\n",
        "GenAI_train_labels = label_encoder.transform(GenAI_train_labels)\n",
        "GenAI_val_labels = label_encoder.transform(GenAI_val_labels)\n",
        "GenAI_test_labels = label_encoder.transform(GenAI_test_labels)\n",
        "\n",
        "def calculate_metrics_and_confusion(pipeline, texts, labels, split_name):\n",
        "    predictions = pipeline.predict(texts)\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    precision = precision_score(labels, predictions, average='weighted')\n",
        "    recall = recall_score(labels, predictions, average='weighted')\n",
        "    f1 = f1_score(labels, predictions, average='weighted')\n",
        "    confusion = pd.crosstab(pd.Series(labels, name='Actual'), pd.Series(predictions, name='Predicted'))\n",
        "\n",
        "    print(f\"\\n{split_name} Metrics:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(confusion)\n",
        "\n",
        "    return {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}, confusion\n",
        "\n",
        "# Train and evaluate sklearn model on SemEval dataset\n",
        "print(\"\\nTraining and evaluating sklearn model on SemEval dataset...\")\n",
        "train_metrics, val_metrics, test_metrics, confusion_matrices = {}, {}, {}, {}\n",
        "\n",
        "# Create pipeline and train on SemEval\n",
        "pipeline = make_pipeline(\n",
        "    CountVectorizer(preprocessor=preprocess_text),\n",
        "    MultinomialNB()\n",
        ")\n",
        "\n",
        "# Measure training time\n",
        "start_time = time.time()\n",
        "pipeline.fit(SemEval_train_texts, SemEval_train_labels)\n",
        "training_time = time.time() - start_time\n",
        "print(f\"Training completed in {training_time:.2f} seconds.\")\n",
        "\n",
        "# Calculate and print metrics for each data split\n",
        "train_metrics['SemEval'], confusion_matrices['SemEval_train'] = calculate_metrics_and_confusion(\n",
        "    pipeline, SemEval_train_texts, SemEval_train_labels, \"Training Set\"\n",
        ")\n",
        "val_metrics['SemEval'], confusion_matrices['SemEval_val'] = calculate_metrics_and_confusion(\n",
        "    pipeline, SemEval_val_texts, SemEval_val_labels, \"Validation Set\"\n",
        ")\n",
        "test_metrics['SemEval'], confusion_matrices['SemEval_test'] = calculate_metrics_and_confusion(\n",
        "    pipeline, SemEval_test_texts, SemEval_test_labels, \"Test Set\"\n",
        ")\n",
        "\n",
        "# Repeat for GenAI dataset\n",
        "print(\"\\nTraining and evaluating sklearn model on GenAI dataset...\")\n",
        "\n",
        "# Measure training time\n",
        "start_time = time.time()\n",
        "pipeline.fit(GenAI_train_texts, GenAI_train_labels)\n",
        "training_time = time.time() - start_time\n",
        "print(f\"Training completed in {training_time:.2f} seconds.\")\n",
        "\n",
        "train_metrics['GenAI'], confusion_matrices['GenAI_train'] = calculate_metrics_and_confusion(\n",
        "    pipeline, GenAI_train_texts, GenAI_train_labels, \"Training Set\"\n",
        ")\n",
        "val_metrics['GenAI'], confusion_matrices['GenAI_val'] = calculate_metrics_and_confusion(\n",
        "    pipeline, GenAI_val_texts, GenAI_val_labels, \"Validation Set\"\n",
        ")\n",
        "test_metrics['GenAI'], confusion_matrices['GenAI_test'] = calculate_metrics_and_confusion(\n",
        "    pipeline, GenAI_test_texts, GenAI_test_labels, \"Test Set\"\n",
        ")\n",
        "\n",
        "# Summary of all results\n",
        "print(\"\\nResults Summary:\")\n",
        "print(\"\\nSemEval Dataset Metrics:\")\n",
        "print(\"Training Set Metrics:\", train_metrics['SemEval'])\n",
        "print(\"Validation Set Metrics:\", val_metrics['SemEval'])\n",
        "print(\"Test Set Metrics:\", test_metrics['SemEval'])\n",
        "\n",
        "print(\"\\nGenAI Dataset Metrics:\")\n",
        "print(\"Training Set Metrics:\", train_metrics['GenAI'])\n",
        "print(\"Validation Set Metrics:\", val_metrics['GenAI'])\n",
        "print(\"Test Set Metrics:\", test_metrics['GenAI'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Custom Implementation (similar to assignment)"
      ],
      "metadata": {
        "id": "5By15cBDitJa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implements a custom Naive Bayes algorithm from scratch, involving manual steps like vocabulary building, log-likelihood calculation, and evaluation.\n",
        "\n",
        "We expect it to take slightly longer and be computationally heavier due to manual processing and likelihood calculations."
      ],
      "metadata": {
        "id": "5P-ScaeLkXp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "import math\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from tqdm import tqdm\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import nltk\n",
        "import pandas as pd\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Constants\n",
        "SMOOTHING_FACTOR = 0.2\n",
        "\n",
        "# Load Data Functions\n",
        "def get_texts_labels(file_path):\n",
        "    \"\"\"Load texts and labels from JSONL file\"\"\"\n",
        "    texts, labels = [], []\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            record = json.loads(line.strip())\n",
        "            texts.append(record['text'])\n",
        "            labels.append(record['label'])\n",
        "    return texts, labels\n",
        "\n",
        "def get_texts(file_path):\n",
        "    \"\"\"Load only texts from JSONL file\"\"\"\n",
        "    texts = []\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            record = json.loads(line.strip())\n",
        "            texts.append(record['text'])\n",
        "    return texts\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Preprocess text while preserving important features for detection\"\"\"\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    text = text.lower()\n",
        "    text = text.replace('\\n', ' ')\n",
        "    text = re.sub(r'[^a-zA-Z\\s.,!?]', '', text)\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words or word in '.,!?'])\n",
        "\n",
        "    return text\n",
        "\n",
        "def train_and_evaluate_custom(train_texts, train_labels, val_texts, val_labels, test_texts, test_labels):\n",
        "    \"\"\"Train and evaluate using custom Naive Bayes implementation for train, validation, and test sets\"\"\"\n",
        "    # Measure training time\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Create training dataframe\n",
        "    train_df = pd.DataFrame({\n",
        "        'text': [preprocess_text(text) for text in train_texts],\n",
        "        'label': train_labels\n",
        "    })\n",
        "\n",
        "    # Build vocabularies\n",
        "    full_vocab, human_vocab, machine_vocab = build_vocabulary(train_df)\n",
        "\n",
        "    # Calculate likelihood\n",
        "    likelihood = calculate_log_likelihood(full_vocab, human_vocab, machine_vocab)\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    print(f\"Training completed in {training_time:.2f} seconds.\")\n",
        "\n",
        "    # Evaluate on train, validation, and test sets\n",
        "    results_train = evaluate_model(pd.DataFrame({'text': [preprocess_text(text) for text in train_texts], 'label': train_labels}), likelihood)\n",
        "    results_val = evaluate_model(pd.DataFrame({'text': [preprocess_text(text) for text in val_texts], 'label': val_labels}), likelihood)\n",
        "    results_test = evaluate_model(pd.DataFrame({'text': [preprocess_text(text) for text in test_texts], 'label': test_labels}), likelihood)\n",
        "\n",
        "    return results_train, results_val, results_test, likelihood\n",
        "\n",
        "def build_vocabulary(df):\n",
        "    \"\"\"Build vocabularies for human and machine text\"\"\"\n",
        "    human_vocab = {}\n",
        "    machine_vocab = {}\n",
        "    full_vocab = {}\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        words = row['text'].split()\n",
        "        for word in words:\n",
        "            full_vocab[word] = full_vocab.get(word, 0) + 1\n",
        "            if row['label'] == 0:  # human\n",
        "                human_vocab[word] = human_vocab.get(word, 0) + 1\n",
        "            else:  # machine\n",
        "                machine_vocab[word] = machine_vocab.get(word, 0) + 1\n",
        "\n",
        "    return full_vocab, human_vocab, machine_vocab\n",
        "\n",
        "def calculate_log_likelihood(vocab, human_vocab, machine_vocab, smoothing_factor=SMOOTHING_FACTOR):\n",
        "    \"\"\"Calculate log likelihood probabilities\"\"\"\n",
        "    likelihood = {}\n",
        "\n",
        "    number_instances_human = sum(human_vocab.values())\n",
        "    number_instances_machine = sum(machine_vocab.values())\n",
        "    number_types = len(vocab)\n",
        "\n",
        "    for word, count in vocab.items():\n",
        "        likelihood[word] = {}\n",
        "\n",
        "        human_word_count = human_vocab.get(word, 0)\n",
        "        machine_word_count = machine_vocab.get(word, 0)\n",
        "\n",
        "        human_probability = (human_word_count + smoothing_factor) / (number_instances_human + smoothing_factor * number_types)\n",
        "        machine_probability = (machine_word_count + smoothing_factor) / (number_instances_machine + smoothing_factor * number_types)\n",
        "\n",
        "        likelihood[word]['human'] = math.log(human_probability)\n",
        "        likelihood[word]['machine'] = math.log(machine_probability)\n",
        "\n",
        "    return likelihood\n",
        "\n",
        "def evaluate_model(df, likelihood):\n",
        "    \"\"\"Evaluate custom model performance\"\"\"\n",
        "    metrics = {\n",
        "        'true_positives': 0,\n",
        "        'false_positives': 0,\n",
        "        'true_negatives': 0,\n",
        "        'false_negatives': 0\n",
        "    }\n",
        "\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        true_label = row['label']\n",
        "        predicted_class, _ = classify_text(row['text'], likelihood, 0.5, 0.5)\n",
        "        predicted_label = 1 if predicted_class == 'machine' else 0\n",
        "\n",
        "        y_true.append(true_label)\n",
        "        y_pred.append(predicted_label)\n",
        "\n",
        "        if true_label == 1 and predicted_label == 1:\n",
        "            metrics['true_positives'] += 1\n",
        "        elif true_label == 1 and predicted_label == 0:\n",
        "            metrics['false_negatives'] += 1\n",
        "        elif true_label == 0 and predicted_label == 0:\n",
        "            metrics['true_negatives'] += 1\n",
        "        elif true_label == 0 and predicted_label == 1:\n",
        "            metrics['false_positives'] += 1\n",
        "\n",
        "    tp = metrics['true_positives']\n",
        "    fp = metrics['false_positives']\n",
        "    tn = metrics['true_negatives']\n",
        "    fn = metrics['false_negatives']\n",
        "\n",
        "    metrics['accuracy'] = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0\n",
        "    metrics['precision'] = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    metrics['recall'] = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    metrics['f1'] = 2 * (metrics['precision'] * metrics['recall']) / (metrics['precision'] + metrics['recall']) if (metrics['precision'] + metrics['recall']) > 0 else 0\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    metrics['confusion_matrix'] = cm\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def classify_text(text, likelihood, human_prior, machine_prior):\n",
        "    \"\"\"Classify text using custom implementation\"\"\"\n",
        "    tokens = preprocess_text(text).split()\n",
        "\n",
        "    log_score_human = math.log(human_prior)\n",
        "    log_score_machine = math.log(machine_prior)\n",
        "\n",
        "    for token in tokens:\n",
        "        if token in likelihood:\n",
        "            log_score_human += likelihood[token]['human']\n",
        "            log_score_machine += likelihood[token]['machine']\n",
        "        else:\n",
        "            unseen_adjustment = math.log(SMOOTHING_FACTOR / (len(likelihood) + 1))\n",
        "            log_score_human += unseen_adjustment\n",
        "            log_score_machine += unseen_adjustment\n",
        "\n",
        "    predicted_class = 'human' if log_score_human > log_score_machine else 'machine'\n",
        "    scores = {'human': log_score_human, 'machine': log_score_machine}\n",
        "\n",
        "    return predicted_class, scores\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Define file paths\n",
        "    file_path = '/content/drive/MyDrive/project/COMP6781_MNBvBERT_MGTDetect-main'\n",
        "    SemEval_train_file_path = file_path + '/SemEval_data/subtaskA_train_monolingual.jsonl'\n",
        "    SemEval_val_file_path = file_path + '/SemEval_data/subtaskA_dev_monolingual.jsonl'\n",
        "    SemEval_test_file_path = file_path + '/SemEval_data/subtaskA_monolingual_test1.jsonl'\n",
        "    GenAI_train_file_path = file_path + '/GenAI_data/en_train.jsonl'\n",
        "    GenAI_val_file_path = file_path + '/GenAI_data/en_dev.jsonl'\n",
        "    GenAI_test_file_path = file_path + '/GenAI_data/en_test.jsonl'\n",
        "\n",
        "    # Load datasets\n",
        "    print(\"Loading datasets...\")\n",
        "    SemEval_train_texts, SemEval_train_labels = get_texts_labels(SemEval_train_file_path)\n",
        "    SemEval_val_texts, SemEval_val_labels = get_texts_labels(SemEval_val_file_path)\n",
        "    SemEval_test_texts, SemEval_test_labels = get_texts_labels(SemEval_test_file_path)  # Load test set\n",
        "\n",
        "    GenAI_train_texts, GenAI_train_labels = get_texts_labels(GenAI_train_file_path)\n",
        "    GenAI_val_texts, GenAI_val_labels = get_texts_labels(GenAI_val_file_path)\n",
        "\n",
        "    # Step 1: Split 10% of the GenAI training set for testing and remove it from the training data\n",
        "    GenAI_train_texts, GenAI_test_texts, GenAI_train_labels, GenAI_test_labels = train_test_split(\n",
        "        GenAI_train_texts, GenAI_train_labels, test_size=0.1, stratify=GenAI_train_labels, random_state=42\n",
        "    )\n",
        "\n",
        "    # Step 2: Stratified Sampling to Match SemEval Dataset Size\n",
        "    train_data, _ = train_test_split(\n",
        "        list(zip(GenAI_train_texts, GenAI_train_labels)),\n",
        "        train_size=len(SemEval_train_texts),\n",
        "        stratify=GenAI_train_labels,\n",
        "        random_state=42\n",
        "    )\n",
        "    GenAI_train_texts, GenAI_train_labels = map(list, zip(*train_data))\n",
        "\n",
        "    # Step 3: Preprocess the training and test sets (train, validation, test for both)\n",
        "    print(\"Preprocessing and training model for SemEval\")\n",
        "    train_results, val_results, test_results, likelihood = train_and_evaluate_custom(\n",
        "        SemEval_train_texts, SemEval_train_labels,\n",
        "        SemEval_val_texts, SemEval_val_labels,\n",
        "        SemEval_test_texts, SemEval_test_labels\n",
        "    )\n",
        "\n",
        "\n",
        "    # Print Results\n",
        "    print(\"\\nResults Summary:\")\n",
        "    print(\"\\nSemEval Dataset Metrics:\")\n",
        "    print(\"Training Results:\", train_results)\n",
        "    print(\"Confusion Matrix for Training Set:\")\n",
        "    print(train_results['confusion_matrix'])\n",
        "    print(\"Validation Results:\", val_results)\n",
        "    print(\"Confusion Matrix for Validation Set:\")\n",
        "    print(val_results['confusion_matrix'])\n",
        "    print(\"Test Results:\", test_results)\n",
        "    print(\"Confusion Matrix for Test Set:\")\n",
        "    print(test_results['confusion_matrix'])\n",
        "\n",
        "    # Step 4: Preprocess the training and test sets (train, validation, test for both)\n",
        "    print(\"Preprocessing and training model for GenAI\")\n",
        "    train_results, val_results, test_results, likelihood = train_and_evaluate_custom(\n",
        "        GenAI_train_texts, GenAI_train_labels,\n",
        "        GenAI_val_texts, GenAI_val_labels,\n",
        "        GenAI_test_texts, GenAI_test_labels\n",
        "    )\n",
        "\n",
        "    # Print Results\n",
        "    print(\"\\nResults Summary:\")\n",
        "    print(\"\\nGenAI Dataset Metrics:\")\n",
        "    print(\"Training Results:\", train_results)\n",
        "    print(\"Confusion Matrix for Training Set:\")\n",
        "    print(train_results['confusion_matrix'])\n",
        "    print(\"Validation Results:\", val_results)\n",
        "    print(\"Confusion Matrix for Validation Set:\")\n",
        "    print(val_results['confusion_matrix'])\n",
        "    print(\"Test Results:\", test_results)\n",
        "    print(\"Confusion Matrix for Test Set:\")\n",
        "    print(test_results['confusion_matrix'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVwY-uo9goG6",
        "outputId": "0a76f489-a71d-4aec-8ad0-f2564516db4f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading datasets...\n",
            "Preprocessing and training model for SemEval\n",
            "Training completed in 144.67 seconds.\n",
            "\n",
            "Results Summary:\n",
            "\n",
            "SemEval Dataset Metrics:\n",
            "Training Results: {'true_positives': 47475, 'false_positives': 18562, 'true_negatives': 44789, 'false_negatives': 8931, 'accuracy': 0.7704267808979851, 'precision': 0.7189151536259976, 'recall': 0.8416657802361451, 'f1': 0.7754628684367421, 'confusion_matrix': array([[44789, 18562],\n",
            "       [ 8931, 47475]])}\n",
            "Confusion Matrix for Training Set:\n",
            "[[44789 18562]\n",
            " [ 8931 47475]]\n",
            "Validation Results: {'true_positives': 1364, 'false_positives': 1259, 'true_negatives': 1241, 'false_negatives': 1136, 'accuracy': 0.521, 'precision': 0.5200152497140679, 'recall': 0.5456, 'f1': 0.5325004879953152, 'confusion_matrix': array([[1241, 1259],\n",
            "       [1136, 1364]])}\n",
            "Confusion Matrix for Validation Set:\n",
            "[[1241 1259]\n",
            " [1136 1364]]\n",
            "Test Results: {'true_positives': 14752, 'false_positives': 1935, 'true_negatives': 14337, 'false_negatives': 3248, 'accuracy': 0.8487686741363212, 'precision': 0.8840414694073231, 'recall': 0.8195555555555556, 'f1': 0.8505780263499294, 'confusion_matrix': array([[14337,  1935],\n",
            "       [ 3248, 14752]])}\n",
            "Confusion Matrix for Test Set:\n",
            "[[14337  1935]\n",
            " [ 3248 14752]]\n",
            "Preprocessing and training model for GenAI\n",
            "Training completed in 80.97 seconds.\n",
            "\n",
            "Results Summary:\n",
            "\n",
            "GenAI Dataset Metrics:\n",
            "Training Results: {'true_positives': 58826, 'false_positives': 6038, 'true_negatives': 38848, 'false_negatives': 16045, 'accuracy': 0.8156015932262832, 'precision': 0.9069129255056734, 'recall': 0.7856980673424958, 'f1': 0.8419651483164562, 'confusion_matrix': array([[38848,  6038],\n",
            "       [16045, 58826]])}\n",
            "Confusion Matrix for Training Set:\n",
            "[[38848  6038]\n",
            " [16045 58826]]\n",
            "Validation Results: {'true_positives': 113944, 'false_positives': 21881, 'true_negatives': 76447, 'false_negatives': 49486, 'accuracy': 0.7273550378593968, 'precision': 0.8389030001840604, 'recall': 0.6972036957718901, 'f1': 0.7615177691266646, 'confusion_matrix': array([[ 76447,  21881],\n",
            "       [ 49486, 113944]])}\n",
            "Confusion Matrix for Validation Set:\n",
            "[[ 76447  21881]\n",
            " [ 49486 113944]]\n",
            "Test Results: {'true_positives': 26636, 'false_positives': 5155, 'true_negatives': 17737, 'false_negatives': 11549, 'accuracy': 0.7265091605678079, 'precision': 0.8378471894561353, 'recall': 0.6975513945266466, 'f1': 0.7612895850005716, 'confusion_matrix': array([[17737,  5155],\n",
            "       [11549, 26636]])}\n",
            "Confusion Matrix for Test Set:\n",
            "[[17737  5155]\n",
            " [11549 26636]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Additional necessary imports\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def hyperparameter_tuning_smoothing(train_texts, train_labels, val_texts, val_labels, test_texts, test_labels, smoothing_factors):\n",
        "    \"\"\"Tune hyperparameter smoothing factor and evaluate the performance\"\"\"\n",
        "    results = {}\n",
        "\n",
        "    for smoothing_factor in smoothing_factors:\n",
        "        print(f\"Evaluating with smoothing factor: {smoothing_factor}\")\n",
        "\n",
        "        # Train and evaluate the model with the current smoothing factor\n",
        "        train_results, val_results, test_results, likelihood = train_and_evaluate_custom(\n",
        "            train_texts, train_labels,\n",
        "            val_texts, val_labels,\n",
        "            test_texts, test_labels,\n",
        "            smoothing_factor=smoothing_factor  # Pass smoothing factor as argument\n",
        "        )\n",
        "\n",
        "        # Save the results for comparison\n",
        "        results[smoothing_factor] = {\n",
        "            'train': train_results,\n",
        "            'val': val_results,\n",
        "            'test': test_results\n",
        "        }\n",
        "\n",
        "        # Print metrics for this smoothing factor\n",
        "        print(f\"Training Accuracy: {train_results['accuracy']}\")\n",
        "        print(f\"Validation Accuracy: {val_results['accuracy']}\")\n",
        "        print(f\"Test Accuracy: {test_results['accuracy']}\")\n",
        "\n",
        "    # Find the smoothing factor with the best performance on the test set\n",
        "    best_smoothing_factor = max(results, key=lambda x: results[x]['test']['accuracy'])\n",
        "    print(f\"Best smoothing factor based on test accuracy: {best_smoothing_factor}\")\n",
        "\n",
        "    return results, best_smoothing_factor\n",
        "\n",
        "def train_and_evaluate_custom(train_texts, train_labels, val_texts, val_labels, test_texts, test_labels, smoothing_factor=0.2):\n",
        "    \"\"\"Train and evaluate using custom Naive Bayes implementation for train, validation, and test sets\"\"\"\n",
        "    # Create training dataframe\n",
        "    train_df = pd.DataFrame({\n",
        "        'text': [preprocess_text(text) for text in train_texts],\n",
        "        'label': train_labels\n",
        "    })\n",
        "\n",
        "    # Build vocabularies\n",
        "    full_vocab, human_vocab, machine_vocab = build_vocabulary(train_df)\n",
        "\n",
        "    # Calculate likelihood with smoothing factor\n",
        "    likelihood = calculate_log_likelihood(full_vocab, human_vocab, machine_vocab, smoothing_factor=smoothing_factor)\n",
        "\n",
        "    # Evaluate on train, validation, and test sets\n",
        "    results_train = evaluate_model(pd.DataFrame({'text': [preprocess_text(text) for text in train_texts], 'label': train_labels}), likelihood)\n",
        "    results_val = evaluate_model(pd.DataFrame({'text': [preprocess_text(text) for text in val_texts], 'label': val_labels}), likelihood)\n",
        "    results_test = evaluate_model(pd.DataFrame({'text': [preprocess_text(text) for text in test_texts], 'label': test_labels}), likelihood)\n",
        "\n",
        "    return results_train, results_val, results_test, likelihood\n",
        "\n",
        "\n",
        "# Main execution for Hyperparameter Tuning\n",
        "if __name__ == \"__main__\":\n",
        "    # Define smoothing factors to evaluate\n",
        "    smoothing_factors = [0.1, 0.2, 0.5, 1.0, 2.0, 3.0]\n",
        "\n",
        "    # Load datasets\n",
        "    print(\"Loading datasets...\")\n",
        "    SemEval_train_texts, SemEval_train_labels = get_texts_labels(SemEval_train_file_path)\n",
        "    SemEval_val_texts, SemEval_val_labels = get_texts_labels(SemEval_val_file_path)\n",
        "    SemEval_test_texts, SemEval_test_labels = get_texts_labels(SemEval_test_file_path)\n",
        "\n",
        "    GenAI_train_texts, GenAI_train_labels = get_texts_labels(GenAI_train_file_path)\n",
        "    GenAI_val_texts, GenAI_val_labels = get_texts_labels(GenAI_val_file_path)\n",
        "\n",
        "    # Step 1: Split 10% of the GenAI training set for testing and remove it from the training data\n",
        "    GenAI_train_texts, GenAI_test_texts, GenAI_train_labels, GenAI_test_labels = train_test_split(\n",
        "        GenAI_train_texts, GenAI_train_labels, test_size=0.1, stratify=GenAI_train_labels, random_state=42\n",
        "    )\n",
        "\n",
        "    # Step 2: Stratified Sampling to Match SemEval Dataset Size\n",
        "    train_data, _ = train_test_split(\n",
        "        list(zip(GenAI_train_texts, GenAI_train_labels)),\n",
        "        train_size=len(SemEval_train_texts),\n",
        "        stratify=GenAI_train_labels,\n",
        "        random_state=42\n",
        "    )\n",
        "    GenAI_train_texts, GenAI_train_labels = map(list, zip(*train_data))\n",
        "\n",
        "    # Step 3: Hyperparameter tuning with smoothing factors for SemEval dataset\n",
        "    print(\"Tuning hyperparameters for SemEval dataset...\")\n",
        "    semeval_results, best_smoothing_factor_semeval = hyperparameter_tuning_smoothing(\n",
        "        SemEval_train_texts, SemEval_train_labels,\n",
        "        SemEval_val_texts, SemEval_val_labels,\n",
        "        SemEval_test_texts, SemEval_test_labels,\n",
        "        smoothing_factors\n",
        "    )\n",
        "\n",
        "    # Step 4: Hyperparameter tuning with smoothing factors for GenAI dataset\n",
        "    print(\"Tuning hyperparameters for GenAI dataset...\")\n",
        "    genai_results, best_smoothing_factor_genai = hyperparameter_tuning_smoothing(\n",
        "        GenAI_train_texts, GenAI_train_labels,\n",
        "        GenAI_val_texts, GenAI_val_labels,\n",
        "        GenAI_test_texts, GenAI_test_labels,\n",
        "        smoothing_factors\n",
        "    )\n",
        "\n",
        "    # Print final best results\n",
        "    print(\"\\nBest smoothing factor for SemEval:\", best_smoothing_factor_semeval)\n",
        "    print(\"\\nBest smoothing factor for GenAI:\", best_smoothing_factor_genai)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQYjzftRiak1",
        "outputId": "5cf90780-55cc-461c-8ce0-a1fa68e76ce6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading datasets...\n",
            "Tuning hyperparameters for SemEval dataset...\n",
            "Evaluating with smoothing factor: 0.1\n",
            "Training Accuracy: 0.7824928814182052\n",
            "Validation Accuracy: 0.5204\n",
            "Test Accuracy: 0.8479808590102708\n",
            "Evaluating with smoothing factor: 0.2\n",
            "Training Accuracy: 0.7704267808979851\n",
            "Validation Accuracy: 0.521\n",
            "Test Accuracy: 0.8487686741363212\n",
            "Evaluating with smoothing factor: 0.5\n",
            "Training Accuracy: 0.7538682498726588\n",
            "Validation Accuracy: 0.5204\n",
            "Test Accuracy: 0.8488270308123249\n",
            "Evaluating with smoothing factor: 1.0\n",
            "Training Accuracy: 0.7405913641791294\n",
            "Validation Accuracy: 0.5206\n",
            "Test Accuracy: 0.8466970121381886\n",
            "Evaluating with smoothing factor: 2.0\n",
            "Training Accuracy: 0.7223878353666174\n",
            "Validation Accuracy: 0.5186\n",
            "Test Accuracy: 0.8380018674136321\n",
            "Evaluating with smoothing factor: 3.0\n",
            "Training Accuracy: 0.7078918142572042\n",
            "Validation Accuracy: 0.5182\n",
            "Test Accuracy: 0.8260387488328664\n",
            "Best smoothing factor based on test accuracy: 0.5\n",
            "Tuning hyperparameters for GenAI dataset...\n",
            "Evaluating with smoothing factor: 0.1\n",
            "Training Accuracy: 0.8295297978406273\n",
            "Validation Accuracy: 0.7305488275429978\n",
            "Test Accuracy: 0.7292434140511158\n",
            "Evaluating with smoothing factor: 0.2\n",
            "Training Accuracy: 0.8156015932262832\n",
            "Validation Accuracy: 0.7273550378593968\n",
            "Test Accuracy: 0.7265091605678079\n",
            "Evaluating with smoothing factor: 0.5\n",
            "Training Accuracy: 0.7975984702355603\n",
            "Validation Accuracy: 0.7243790065633142\n",
            "Test Accuracy: 0.723463824352866\n",
            "Evaluating with smoothing factor: 1.0\n",
            "Training Accuracy: 0.7850313551608674\n",
            "Validation Accuracy: 0.724260576563085\n",
            "Test Accuracy: 0.7229726410923916\n",
            "Evaluating with smoothing factor: 2.0\n",
            "Training Accuracy: 0.77816745576459\n",
            "Validation Accuracy: 0.7272557094721078\n",
            "Test Accuracy: 0.7264927877924587\n",
            "Evaluating with smoothing factor: 3.0\n",
            "Training Accuracy: 0.7776580909675426\n",
            "Validation Accuracy: 0.731351095286486\n",
            "Test Accuracy: 0.7315192298246476\n",
            "Best smoothing factor based on test accuracy: 3.0\n",
            "\n",
            "Best smoothing factor for SemEval: 0.5\n",
            "\n",
            "Best smoothing factor for GenAI: 3.0\n"
          ]
        }
      ]
    }
  ]
}